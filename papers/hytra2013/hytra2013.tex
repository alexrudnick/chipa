\documentclass[11pt]{article}
\usepackage{acl2013}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{float}
\floatstyle{boxed}
\restylefloat{figure}
\lstset{
language=Python,
basicstyle=\small\sffamily,
numbers=none,
numberstyle=\tiny,
frame=tb,
columns=fullflexible,
showstringspaces=false
}
%\setlength\titlebox{6.5cm}    % You can expand the title box if you
% really have to

%% \title{Cross-Language WSD for Hybrid MT as Sequence Labeling}
\title{Lexical Selection for Hybrid MT with Sequence Labeling}


\author{Alex Rudnick and Michael Gasser\\
        Indiana University, School of Informatics and Computing \\
        {\tt \{alexr,gasser\}@indiana.edu}}

\date{}

\begin{document}
\maketitle
\begin{abstract}
We present initial work on an inexpensive approach for building
large-vocabulary lexical selection modules for hybrid RBMT systems by framing
lexical selection as a sequence labeling problem. We submit that Maximum
Entropy Markov Models (MEMMs) are a sensible formalism for this problem, due to
their ability to take into account many features of the source text, and show
how we can build a combination MEMM/HMM system that allows MT system
implementors flexibility regarding which words have their lexical choices
modeled with classifiers. We present initial results showing successful use of
this system both in translating English to Spanish and Spanish to Guarani.
\end{abstract}

\section{Introduction}
Lexical ambiguity presents a serious challenge for rule-based MT systems, since
many words have several possible translations in a given target language, and
more than one of them may be syntactically valid in context. A translation
system must choose a translation for each word or phrase in the input sentence,
and simply taking the most common translation will often fail, as a word in the
source language may have translations in the target language with significantly
different meanings. Even when chosing among near-synonyms, we would like to
respect selectional preferences and common collocations to produce
natural-sounding output text.

Writing lexical selection rules by hand is tedious and error-prone; even if
informants familiar with both languages are available, they may not be able to
enumerate the contexts under which they would choose one translation
alternative over another. Thus we would like to learn from corpora where
possible. 

Framing the resolution of lexical ambiguities as an explicit classification
task has a long history, dating back at least as to early SMT work at IBM
\cite{Brown91word-sensedisambiguation}.  More recently, Carpuat and Wu have
shown how to use word-sense disambiguation techniques to improve modern
phrase-based SMT systems \cite{carpuatpsd}, even though the language model and
phrase-tables of these systems can mitigate the problem of lexical ambiguities
somewhat. Treating lexical selection as a word-sense disambiguation problem, in
which the sense inventory for each source-language word is its set of possible
translations, is often called cross-lingual WSD (CL-WSD). This framing has
received enough attention to warrant shared tasks at recent SemEvals; the most
recent running of the task is described in \cite{task10}.

Intuitively, machine translation implies an ``all-words" WSD task, both in that
we need to choose a translation for every word or phrase in the source
sentence, and in that we want to produce a \emph{sequence} of translations that
makes sense taken together. In this work, we begin to explore CL-WSD not just
as a classification task, but as one of sequence labeling. Treating the lexical
selection task as sequence labeling is in some sense like building a simplified
SMT system, although here we disregard reordering and the multi-word phrases of
the source-language.  While lexical selection by itself does not make a
complete MT system, for languages with relatively close syntax, it could be
useful for extracting the gist of a text, something like Resnik's early
word-by-word translation baseline \cite{resnik:aaai}.

This is work in progress and our code is currently ``research-quality", but we
are developing the software\footnote{Available at \\
\url{http://github.com/alexrudnick/clwsd}} in the open, with the intention of
using it with free RBMT systems and producing an easily reusable package as the
system matures.

\section{Related Work}
To our knowledge, there has not been work specifically on sequence labeling
applied to lexical selection for rule-based MT systems. However, 
there has been work recently on using WSD techniques for translation into
lower-resourced languages, such as the English-Slovene language pair, as in 
\cite{vintar-fivser-vrvsvcaj:2012:ESIRMT-HyTra2012}. 

The Apertium team has a particular practical interest in improving lexical
selection in rule-based machine translation; they recently have been developing
a new system, described in \cite{tyers-fst}, that learns finite-state
transducers for lexical selection from the available parallel corpora. It is
intended to be both very fast, for use in practical translation systems, and
produce lexical selection rules that are understandable and modifiable by
humans.

There has also been work on framing all-words WSD as a sequence labeling
problem. Particularly, Molina \textit{et al.}
\shortcite{DBLP:conf/iberamia/MolinaPS02} have made use of HMMs for all-words
WSD in a monolingual setting.

\section{Sequence Labeling with HMMs}
We can imagine the lexical selection task as analogous to part-of-speech
tagging, in that we want to predict an appropriate tag for each word in an
input sentence. Here instead of POS tags, our output labels are words or
phrases in the target language. In the generative model of an HMM POS tagger,
we say, for some input sentence $W$, and a candidate sequence of tags $T$:
$ P(W, T) = P(T) * P(W|T) $.

And using the (first-order) Markov assumption, we approximate $P(T)$ as 
$P(T) = \prod_{i} P(t_i | t_{i-1})$, where $i$ denotes each index in the
sentence. Then we imagine that each word $w_i$ is generated by the
corresponding (unobserved) tag $t_i$, through the emission probabilities
$P(w|t)$.

For lexical selection, instead of having a tag set, we have a vocabulary of
target-language words and phrases. We imagine the same generative model, which
includes transition probabilities over target-language words or phrases, and
emission probabilities with which source-language words $s$ are emitted from
the hidden target-language states, $P(s|t)$. This generative model is somewhat
less intuitive for CL-WSD than for POS-tagging, in that it requires the
target-language words to be generated in the source order.

Training the transition model (\textit{i.e.}, a language model) for
target-language words or phrases in the source order is straightforward with
sentence-aligned bitext; we want one-to-many alignments in which each source
word corresponds with zero or more target-language words, and we take the
sequence of target-language words aligned with a given source word to be its
label. NULL labels are common; if a source word is not aligned to a target
word, it gets a NULL label. In a similar way, we can learn the emission
probabilities, $P(s|t)$, simply by counting which source words are paired with
which target words and smoothing.

For decoding with this model, we can use the Viterbi algorithm, especially for
a first-order Markov model -- although we must be careful in the inner loops
only to consider the possible target-language words and not the entire
target-language vocabulary. The Viterbi algorithm may still be used with
second- or higher-order models, although it slows down considerably and in the
interest of time we may wish to use an approximate search like beam search.

\section{Sequence Labeling With MEMMs and HMMs}
In contrast with an HMM, an MEMM is a discriminative sequence model, with
which we can calculate the conditional probability $P(T|S)$, using individual
discriminative classifiers that model $P(t_i | F)$ (for some features $F$).
Like an HMM, an MEMM models of transitions over labels, but unlike an HMM, the
input sequence is considered given. This frees us to include any features we
like from the source-language sentence. The ``Markov" aspect of the MEMM is
that, unlike a standard maximum entropy classifier, we can include information
from the previous $k$ labels as features, for a $k$-th order MEMM. So at every
step in the sequence labeling, we want to have a classifier that models 
$P(t_i | S, t_{i-1}...t_{i-k})$, and the probability of a sequence $T$ is just
the product of each of the individual transition probabilities -- there are no
``emission" probabilities, since the input sequence is given.

To avoid the intractable task of building a single classifier that might return
many thousands of different labels, we might choose to build a classifier for
each individual word in the source-language vocabulary, each of which might
produce tens of output labels at most. However, there will be tens or hundreds
of thousands of words in the source-language vocabulary, and most word-types
will only occur very rarely, and it may be too expensive to train and store a
very large number of classifiers.

We would like a way to focus our efforts on some words, but not all, and to
back off to a simpler model when a classifier is not available for a given
word. In our system, we allow users to specify criteria under which a given
source-language word will have its translations explicitly modeled with a
maximum entropy classifier. When training a system, one might choose, for
example, the 100 most common ambiguous words, all words that occur a certain
number of times, or words that are particularly of interest for some other
reason.

The system then, during training, extracts all of the instances of the words
that we want to model with classifiers along with their contexts (extracting
appropriate features for training the classifiers), trains classifiers for
those words, and stores the classifiers in a database for retrieval at
inference time.

The question remains, then, of how to get the probability $P(t_i | F)$ when we
do not have a stored classifier for the corresponding source word $s_i$. In
this case, we back off to the HMM, with which we can calculate $P(t_i |
t_{i-1}...t_{i-k}) * P(s_i | t_i)$. This gives us $P(s_i, t_i |
t_{i-1}...t_{i-k})$, and if we divide this by $P(s_i)$ -- which must be stored
ahead of time -- then we can approximate the conditional probability that we
need to continue the sequence tagging.

For inference with this model, we implemented a beam search rather than the
Viterbi algorithm, for convenience and speed while using a second-order Markov
model.

%%A sketch of the beam search implementation is presented in Figure
%%\ref{fig:beamsearch}.
%%
%%\begin{figure*}
%%\begin{lstlisting}[frame=none]
%%def beam_search(sequence, HMM, source_word_priors, classifiers):
%%    """Search over possible label sequences for the input sequence. Return the
%%       best sequence we find."""
%%    # list of incomplete candidate labelings, with their penalties
%%    candidates = [Candidate([], 0)] # empty label sequence with 0 penalty
%%    for t in range(len(sequence)):
%%        sourceword = sequence[t]
%%        new_candidates = []
%%        for candidate in candidates:
%%            context = candidates.get_context(t) # last k labels
%%            if sourceword in classifiers:
%%                features = extract_features(sequence, t, context)
%%                label_distribution = classifiers[sourceword].prob_classify(features)
%%            else:
%%                label_distribution = Distribution()
%%                for label in get_vocabulary(sourceword):
%%                    label_distribution[label] = (HMM.transition(context, label) +
%%                                                 HMM.emission(sourceword, label) -
%%                                                 source_word_priors[sourceword])
%%            new_candidates = add_new_candidates(label_distribution)
%%        candidates = filter_top_k(new_candidates, BEAMWIDTH)
%%    return get_best(candidates)
%%
%%\end{lstlisting}
%%\caption{Python-style code sketch for combined MEMM/HMM beam search. We assume
%%that \texttt{prob\_classify} returns a mapping from labels to negative
%%log-probabilities, which we interpret as penalties to be minimized.
%%}
%%\label{fig:beamsearch}
%%\end{figure*}

\section{Experiments}
So far, we have evaluated our sequence-labeling system in two different
settings, the English-Spanish subset of a recent SemEval shared task
\cite{task10}, and an all-words prediction task in which we want to predict
every each word in sentences from the Bible.

\subsection{SemEval CL-WSD task}
In the SemEval CL-WSD task, systems must provide translations for twenty
ambiguous English nouns, given a small amount of context, typically a single
sentence. The test set for this task consists of fifty short passages for each
ambiguous word, for a thousand test instances total; each passage contains one
or a few uses of the ambiguous word. For each test passage, the system must
produce a translation of the noun of interest into the target language.  These
translations may be a single word or a short phrase in the target language, and
they should be lemmatized. The task allows systems to produce several output
labels, although the scoring metric encourages producing one best guess, which
is matched against several reference translations provided by human annotators.
The details of the scoring are provided in the task description paper.

For simplicity and comparability with previous work, we trained our system on
the Europarl Intersection corpus, which was provided for developing CL-WSD
systems in the shared task.  The Europarl Intersection is a subset of the
sentences from Europarl \cite{europarl} that are available in English and all
five of the target languages for the task, although for these initial
experiments, we only worked with Spanish. There were 884603 sentences in our
training data.

We preprocess the Europarl training data by tokenizing with the default NLTK
tokenizer \cite{nltkbook}, getting part-of-speech tags for the English text
with the Stanford Tagger \cite{Toutanova03feature-richpart-of-speech}, and
lemmatizing both sides with TreeTagger \cite{Schmid95improvementsin}.  We
aligned the untagged English text with the Spanish text using the Berkeley
Aligner \cite{denero-klein:2007:ACLMain} to get one-to-many alignments from
English to Spanish, since the target-language labels in this setting may be
multi-word phrases. We used nearly the default settings for Berkeley Aligner,
except that we ran 20 iterations each of IBM Model 1 and HMM alignment.

We trained classifiers for all of the test words, and also for any words that
appear more than 500 times in the corpus. The classifiers used the previous two
labels, and all of the tagged, lemmatized words within a three-token window as
features, and were trained with the MEGA Model optimization package
\footnote{\url{http://www.umiacs.umd.edu/~hal/megam/}} and its corresponding
NLTK interface.

At testing time, for each test instance, we used each of the different sequence
labeling methods to label all input sentences, and took the assigned label for
the noun in question as the answer for that instance.

\subsection{All-words Lexical Selection for Spanish-Guarani}
Since we are primarily interested in lexical selection for RBMT systems in
lower-resource settings, we also experiment with translating from Spanish to
Guarani, using the Bible as bitext. In this experiment, we labeled all of the
text in the test set, using each of the different sequence labeling models, and
we report the classification accuracy over the test set.

In preparing the corpus, since different translations of the Bible do not
necessarily have direct correspondences between verse numbers (they are not
unique identifiers across language!), we select only the chapters that contain
the same number of verses in our Spanish and Guarani translations.  This only
leaves 879 chapters, out of 1189 total, for a total of 22828 bitext verses of
roughly one sentence each. We randomly sample 100 verses from the corpus and
set these aside as the test set.

Here we trained the HMM and MEMM as before, but with lemmatized Spanish as the
source language, and the roots of Guarani words as the target.  As Guarani is
much more morphologically rich language than either English or Spanish, this
requires the use of a sophisticated morphological analyzer, which is described
in section \ref{sec:guaranima}. Due to the much smaller data set, in this
setting we stored classifiers for any Spanish word that occurs more than 20
times in the training data.

\section{Morphological Analysis for Guarani}
\label{sec:guaranima}
We analyze the Spanish and Guarani Bible using our in-house morphological
analyzer, originally developed for Ethiopian Semitic languages 
\cite{gasser:eacl09}.
As in other, more familiar, modern
morphological analyzers such as \cite{beesley+karttunen}, analysis in our
system is modeled by cascades of finite-state transducers (FSTs).  To solve the
problem of long-distance dependencies, we extend the basic FST framework using
an idea introduced by Amtrup \shortcite{amtrup:03}.  Amtrup starts with the
well-understood framework of weighted FSTs, familiar from speech recognition.
For speech recognition, FST arcs are weighted with probabilities, and a
successful traversal of a path through a transducer results in a probability
that is the product of the probabilities on the arcs that are traversed, as
well as an output string as in conventional transducers.  Amtrup showed that
probabilities could be replaced by feature structures and multiplication by
unification.  In an FST weighted with feature structures, the result of a
successful traversal is the unification of the feature structure ``weights'' on
the traversed arcs, as well as an output string.  Because a feature structure
is accumulated during the process of transduction, the transducer retains a
sort of memory of where it has been, permitting the incorporation of
long-distance constraints such as those relating the negative prefix and suffix
of Guarani verbs.

In our system, the output of the morphological analysis of a word is a root and
a feature structure representing the grammatical features of the word.  We
implemented separate FSTs for Spanish verbs, for Guarani nouns, and for the two
main categories of Guarani verbs and adjectives.  Since Spanish nouns and
adjectives have very few forms, we simply list the alternatives in the lexicon
for these categories.  For this paper, we are only concerned with the roots of
words in our corpora, so we ignore the grammatical features that are output
with each word.

\floatstyle{plain}
\restylefloat{figure}
\begin{figure*}[t!]
  \begin{center}
  \begin{tabular}{|r|l|r|}
    \hline
    system & features & score (precision) \\
    \hline
     MFS (with tag) &                                 & 24.97 \\
     MFS (without tag) &                              & 23.23 \\
    \hline
     HMM1    & current word, previous label           & 21.17 \\
     HMM2    & current word, previous two labels      & 21.23 \\
     MaxEnt  & three-word window                      & 25.64 \\
     MEMM    & three-word window, previous two labels & \textbf{26.49} \\
    \hline
  \end{tabular}
  \end{center}
\caption{Results for the first experiment; SemEval 2013 CL-WSD task.}
\label{fig:theresults}
\end{figure*}

\floatstyle{plain}
\restylefloat{figure}
\begin{figure*}[t!]
  \begin{center}
  \begin{tabular}{|r|l|r|}
    \hline
    system & features & score (accuracy \%) \\
    \hline
    MFS      &                                        & 60.39  \\
    \hline
     HMM1    & current word, previous label           & 57.40  \\
     HMM2    & current word, previous two labels      & 43.04  \\
     MEMM    & three-word window, previous two labels & \textbf{66.82}  \\
    \hline
  \end{tabular}
  \end{center}
\caption{Results for the second experiment; all-words lexical selection on the
Guarani Bible}
\label{fig:theresults2}
\end{figure*}

\section{Results}
We see the scores for the first experiment in Figure \ref{fig:theresults}. The
first MFS baseline reported (``with tag"), is the baseline in which we always
take the most frequent label for the source word, conditioned on its POS tag.
The other MFS baseline is from the SemEval competition, not conditioned on POS
tag. Perhaps unsurprisingly, here we see part-of-speech tagging doing some of
the lexical disambiguation work. Neither of the HMM systems beat the
most-frequent-sense baseline, but both the non-sequence MaxEnt classifier and
the MEMM system did, suggesting that the previous labels are informative, and
that the beam search finds fairly good label sequences.

The scores for the second experiment are presented in Figure
\ref{fig:theresults2}; here we see similar results. Neither of the HMM systems
beat the MFS baseline, and the trigram model was noticeably worse. The training
set here is probably too sparse to train a good trigram model. The MEMM system,
however, did beat the baseline, posting the highest results.

%%english to Spanish
%%unigrams mean: 24.9745
%%bigrams mean: 21.169999999999998
%%trigrams mean: 21.227
%%
%%now with classifiers
%%maxent mean: 25.64
%%memms mean: 26.49

%% Spanish to Guarani, all words accuracy
%% unigrams
%% accuracy: 0.603943661971831
%% bigrams
%% accuracy: 0.5740845070422536
%% trigrams
%% accuracy: 0.4304225352112676
%% memm: MEMM BEATS MOST FREQUENT SENSE
%% accuracy: 0.668169014084507

\section{Conclusions and Future Work}
We have described a work-in-progress lexical selection system that takes a
sequence labeling approach, and shown some initial successes in using it for
cross-language word sense disambiguation tasks for English to Spanish and
Spanish to Guarani.  We have demonstrated a hybrid sequence labeling strategy
that combines MEMMs and HMMs, which will allow users to set parameters sensibly
for their computational resources and available training data. The initial
results with the MEMM/HMM system are promising.

In future work, we will continue to refine the approach, exploring different
parameter settings, such as beam widths, numbers of classifiers for the MEMM
component, and the effects of different features as input to the classifiers.
We are also interested in making use of multilingual information sources,
as in the work of Lefever and Hoste
\shortcite{lefever-hoste-decock:2011:ACL-HLT2011}. We may also consider more
sophisticated sequence tagging models, such as CRFs
\cite{DBLP:conf/icml/LaffertyMP01}, although we may not have enough training
data to make use of richer models.

Our goal for this work is practical in that we are trying to produce a
primarily-RBMT system for Spanish-Guarani, but we are interested machine
learning methods where applicable; we have a small amount of training data
available and plans to collect more.  At the time of writing, we are working
with a prototype lexical selection system that is not yet integrated into the
rule-based MT engine, but this integration is among our near-term goals.

A limitation of the current design is that we do not yet have a good way to
make use of monolingual training data. In SMT, it is common practice to train a
language model for the target language from a monolingual corpus that is much
larger than the available bitext. There is a substantial amount of available
Guarani text on the Web, and we would gladly make use of it, but our current
model can only be trained on aligned bitext.  Given Guarani text that had been
rearranged into a Spanish-like word order, we could build a better model for
the transition probabilities in the HMM component of the system. It might be
feasible to use a Guarani-language parser and some linguistic knowledge for
this purpose, though acquiring a good parser may present some difficulty.
We will also investigate ways to identify multiword expressions that should be
translated as a unit, rather than word-by-word.

\bibliographystyle{acl.bst}
\bibliography{hytra2013.bib}{}

\end{document}
