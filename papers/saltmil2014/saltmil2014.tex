\documentclass[10pt, a4paper]{article}
\usepackage{lrec2006}
\usepackage{graphicx}
\usepackage{url}
\usepackage{latexsym}
\usepackage{listings}
\usepackage[utf8]{inputenc}

\title{Enhancing a Rule-Based MT System with Cross-Lingual WSD}

%%\name{Author1, Author2, Author3}
%%
%%\address{ Affiliation1, Affiliation2, Affiliation3 \\
%%               Address1, Address2, Address3 \\
%%               author1@xxx.yy, author2@zzz.edu, author3@hhh.com\\}

\abstract{
Lexical ambiguity is a significant problem facing rule-based machine
translation systems, as many words have several possible translations in a
given target language, each of which can be considered a sense of the word from
the source language.
The difficulty of resolving these ambiguities is mitigated for 
statistical machine translation systems for language pairs with large bilingual
corpora, as large n-gram language models and phrase tables containing common
multi-word expressions can encourage coherent word choices.
For most language pairs these resources are not available, so a primarily
rule-based approach becomes attractive.
In cases where some training data is available, though, we can
investigate hybrid RBMT and machine learning approaches, leveraging small and
potentially growing bilingual corpora. In this paper we
describe the integration of statistical cross-lingual word-sense disambiguation
software with SQUOIA, an existing rule-based MT system for the Spanish-Quechua
language pair, and show how it allows us to learn from the available bitext to
make better lexical choices, with very few code changes to the base system. We
also describe Chipa, the new open source CL-WSD software used for these
experiments.
\\ \newline
\Keywords{under-resourced languages, hybrid machine translation, word-sense
disambiguation}}

\begin{document}

\maketitleabstract

\section{Introduction}
Here we report on the development of Chipa, a package for statistical
lexical selection, and on integrating it into
SQUOIA\footnote{\url{http://code.google.com/p/squoia/}}, a rule-based
machine translation system for the Spanish-Quechua language pair.  With very
few code changes to SQUOIA, we were able to make use of the lexical suggestions
provided by Chipa.

The integration enables SQUOIA to take advantage of any available bitext
without significantly changing its design, and to improve its word choices as
additional bitext becomes available. Our initial results also suggest that we
are able to use unsupervised approaches on monolingual Spanish text to further
improve results.

In this paper, we describe the designs of the Chipa and SQUOIA systems, discuss
the data sets used, and give results on both how well Chipa is able to learn
lexical selection classifiers in isolation, and to what extent it is able to
improve the output of SQUOIA on a full Spanish-to-Quechua translation task.

In its current design, SQUOIA makes word choices based on its bilingual
lexicon; the possible translations for a given word or multi-word expression
are retrieved from a dictionary file on demand. If there are several possible
translations for a lexical item, these are passed along the pipeline in hopes
that later stages will make a decision, but if no disambiguation decision is
made, then the first entry retrieved from the lexicon is used. While there are
some rules for lexical selection, they have been written by hand and only cover
a small subset of the vocabulary.

In this work, we supplement these rules with classifiers learned from
Spanish-Quechua bitext. These classifiers make use of regularities that may not
be obvious to human rule-writers, providing improved lexical selections for
any word type with adequate coverage in the training corpus.

Quechua is a group of closely related indigenous American languages spoken in
South America. There are many dialects of Quechua, but SQUOIA focuses on the
Cuzco dialect, spoken around the Peruvian city of Cuzco.  Cuzco Quechua has
several million speakers, and some useful linguistic resources available,
including a small treebank \cite{rios2009quechua}, also produced by the SQUOIA
team.

\section{SQUOIA}
SQUOIA is a shallow-transfer RBMT system based on the
architecture of MATXIN \cite{matxin_2005,matxin}.
The core system relies on a classical transfer approach, and is mostly
rule-based, with a few components based on machine learning.
SQUOIA uses a pipeline approach, both in an abstract architectural sense and in
that its pieces are instantiated as a series of scripts that communicate
via UNIX pipes. Each module performs some transformation on its input and
passes along the updated version to the next stage. Many modules focus on very
particular parts of the representation, leaving most of their input unchanged.

In the first stages, Spanish source sentences are analyzed with off-the-shelf
open-source NLP tools. To analyze the input Spanish text,
SQUOIA uses FreeLing \cite{padro12} for morphological analysis and named-entity
recognition,
Wapiti \cite{lavergne2010practical} for tagging,
and DeSr \cite{attardi-EtAl:2007:EMNLP-CoNLL2007} for parsing.
All of these modules rely on statistical models.

In the next step, the Spanish verbs must be disambiguated in order to assign
them a Quechua verb form for generation: a rule-based module tries to assign a
verb form to each verb chunk based on context information. If the rules fail to
do so due to parsing or tagging errors, the verb is marked as ambiguous and
passed on to an SVM classifier, which assigns a verb form even if the context
of that verb does not unambiguously select a target form. This is among the
most difficult parts of the
translation process, as the grammatical categories encoded in verbs differ
substantially between Spanish and Quechua. In the next step, a lexical transfer
module inserts all possible translations for every word from a bilingual dictionary.
Then a set of rules disambiguates the forms with lexical or morphological
ambiguities. However, this rule-based lexical disambiguation is very limited,
as it is not feasible to cover all possible contexts for every ambiguous word
with rules.

The rest of the system is a classical transfer procedure. A following module
moves syntactic information between the nodes and the chunks in the tree, and
finally, the tree is reordered according to the basic word order in the target
language. In the last step, the Quechua surface forms are morphologically
generated through a finite state transducer.

\section{CL-WSD with Chipa}
Chipa is a system for cross-language word sense disambiguation (CL-WSD)
\footnote{Chipa the software is named for chipa the snack food, popular in many
parts of South America. It is a cheesy bread made from cassava flour, and
often served in a bagel-like shape in Paraguay. Its Portuguese name,
\emph{p\~{a}o de queijo}, may be familiar to readers.}. By
CL-WSD, we mean the problem of assigning labels to polysemous words in
source-language text, where each label is a word or phrase type in the target
language.

This framing of word-sense disambiguation neatly addresses the problem of
choosing an appropriate sense inventory for WSD, which has historically been a
difficult problem for the practical application of WSD systems.
Here the sense distinctions that the CL-WSD system should learn are exactly
those that are lexicalized in the target language.
The CL-WSD framing also sidesteps the ``knowledge
acquisition bottleneck" \cite{agirre2006word}
hampering other work in WSD. While supervised CL-WSD methods typically require
bitext for training, this is more readily available than the sense-annotated
text that would be otherwised required.

The Chipa system 
We have to say: well, here are the options. es1 can translate to qu1 or qu2 or
qu3. Which one of these do you like the best, chipa?
What if a qu word is in the lexicon but it's never seen in the training data?
Just always prefer the words that we've seen?

The chipa system holds all of the available bitext in a database, waiting for
sentences to translate.
On demand, chipa either retrieves or trains (then caches) a classifier
for each word in a sentence to be translated.

Classifiers are trained with the scikit-learn machine learning package
\cite{scikit-learn}, and the software uses various utility functions from NLTK
\cite{nltkbook}.


Notably, many instances of a source-language word may be aligned to NULL in the
bitext; function words especially are likely to simply be dropped in the ...




Open question: how much training data do we need for a word before it helps to
do a classifier?

Proposal:
- if never seen it, just don't disambiguate
- if seen it a few times, take the MFS
- if many times, train a classifier!


* consider: how many instances of a word do we need before it helps to train a
classifier?

Describe the results of initial testing on a held-out test set of es-qu CL-WSD



\subsection{System Integration}
In order to integrate Chipa into SQUOIA, we added a stage to the SQUOIA
pipeline that determines which words in the current input sentence had multiple
possible translations retrieved from the lexicon, and then asks a Chipa
server for translation suggestions, using XMLRPC.

The suggested translations are returned 


according to the system's lexicon, 
are to be disambiguated 

Given the Chipa WSD software and the pipeline design of the SQUOIA MT system,
we were





Chipa uses discriminative classifiers, trained individually for source-language
words, in order to distinguish between the senses corresponding to different
target-language word types.

In this work, we use very simple contextual features extracted from the 

Features extracted include ...
- syntactic features? we should have a parse/chunking handy!
- bag of words over the whole input sentence?
- local context words?
- foreign-language BOW? Maybe we'll try that later?
- sequence models?

a number of features extracted ...


\section{Available bitext and data preparation}
For this work, we use an assortment of Spanish-Quechua bitexts.
While there are not very many available large corpora, there are some.

As sentences alignments are not guaranteed to be clean, we use Robert Moore's
sentence aligner \cite{DBLP:conf/amta/Moore02}, with the default settings.
There were THISMANY sentences in the corpora we used, THISMANY of which are
included in the resulting sentence-aligned bitext.

We performed word-level alignments on the remaining sentences with the Berkeley
aligner, resulting in one-to-many alignments such that each Spanish word may be
aligned to zero or more Quechua words. Spanish multi-word expressions known to
the lexicon were replaced with special tokens to mark that particular
expression.

\section{Learning From Monolingual Data}
While in this work, our target language is under-resourced, we have many
language resources available for the source language. We would to use these to
make better sense of the input text, giving our classifiers clearer signals for
lexical selection in the target language.

One resource for Spanish is the simple abundance of monolingual text. Given
large amounts of Spanish-language text, we can use unsupervised methods to
discover semantic regularities. In order to learn from this text, we apply
Brown clustering \cite{Brown92class-basedn-gram}, which has been used in a
variety of text classification tasks \cite{turian-ratinov-bengio:2010:ACL}.

The Brown clustering algorithm takes as input unannotated text and maps each
word type in the text to a cluster, such that words in the same cluster have
similar usage patterns according to bigram statistics. We can then use this
mapping from words to clusters in our classifiers, adding an additional
annotation for each word that allow the classifiers to find slightly
higher-level abstractions than surface-level words or particular lemmas.
We use a popular open source implementation of Brown clustering
\footnote{\url{https://github.com/percyliang/brown-cluster}}, described in
\cite{Liang05semi-supervisedlearning}, running on both the Spanish side of our
bitext corpus and on the Europarl corpus for Spanish.

\section{Experiments}
%% XXX
We prepared a number of example sentence to be translated, and found that
XXXX\% of them had better translations because of better lexical choice.

Hey, maybe words that are in the Spanish wikipedia article linked to a Quechua
wikipedia article are informative features for selecting those Quechua words?
...

%%\floatstyle{plain}
%%\restylefloat{figure}
%%\begin{figure*}[t!]
%%  \begin{center}
%%  \begin{tabular}{|r|l|r|}
%%    \hline
%%    system & features & score (precision) \\
%%    \hline
%%     MFS (with tag) &                                 & 24.97 \\
%%     MFS (without tag) &                              & 23.23 \\
%%    \hline
%%     HMM1    & current word, previous label           & 21.17 \\
%%     HMM2    & current word, previous two labels      & 21.23 \\
%%     MaxEnt  & three-word window                      & 25.64 \\
%%     MEMM    & three-word window, previous two labels & \textbf{26.49} \\
%%    \hline
%%  \end{tabular}
%%  \end{center}
%%\caption{Results for the first experiment; SemEval 2013 CL-WSD task.}
%%\label{fig:theresults}
%%\end{figure*}
%%
%%%%\floatstyle{plain}
%%%%\restylefloat{figure}

\begin{figure*}[t!]
  \begin{center}
  \begin{tabular}{|r|r|r|r|r|r|}
    \hline
    corpus          & $C=100$ & $C=200$ & $C=500$ & $C=1000$ & $C=2000$ \\
    \hline
    no brown clusters &    & & & &  \\
    training bitext &                            &         &         &          &          \\
    europarl        &                            &         &         &          &          \\
    \hline
  \end{tabular}
  \end{center}
\caption{Results for the \emph{in-vitro} experiment; classification accuracies
over tenfold cross-validation including null-aligned words, as percentages. }
\label{fig:theresults1}
\end{figure*}


\begin{figure*}[t!]
  \begin{center}
  \begin{tabular}{|r|r|r|r|r|r|}
    \hline
    corpus          & $C=100$ & $C=200$ & $C=500$ & $C=1000$ & $C=2000$ \\
    \hline
    no brown clusters &    & & & &  \\
    training bitext &                            &         &         &          &          \\
    europarl        &                            &         &         &          &          \\
    \hline
  \end{tabular}
  \end{center}
\caption{Classification accuracies over tenfold cross-validation, excluding
null-aligned words.}
\label{fig:theresults2}
\end{figure*}


\section{Related Work}
Framing the resolution of lexical ambiguities in machine translation
as an explicit classification
task has a long history, dating back at least to early SMT work at IBM
\cite{Brown91word-sensedisambiguation}.  More recently, Carpuat and Wu have
shown how to use classifiers to improve modern phrase-based SMT systems
\cite{carpuatpsd}.
CL-WSD has received enough attention to warrant shared tasks at recent SemEval
workshops; the most recent running of the task is described in \cite{task10}.
In this task, participants are asked to translate twenty different polysemous
English nouns into five different European languages, in a variety of contexts.

Lefever \emph{et al.}, in work on the ParaSense system
\shortcite{lefever-hoste-decock:2011:ACL-HLT2011}, produced top results for
this task with classifiers trained on local contextual features, with the 
addition of a bag-of-words model of the translation of the complete source
sentence into other (neither the source nor the target) languages. At training
time, the translations of the sentence are extracted from available parallel
corpora, but at training time, the translations must be produced by a
third-party MT system.
This work has not yet, to our knowledge, been integrated into to an MT system
on its own.

Rudnick \emph{et al.} \shortcite{rudnick-liu-gasser:2013:SemEval-2013}
prototyped a system that addresses some of the shortcomings here, requiring
more modest software infrastructure for feature extraction while still allowing
CL-WSD systems to make use of several mutually parallel bitexts that share a
source language.

There has been some work on CL-WSD for translating into indigenous American
languages; an earlier system for Spanish-Guarani made use of sequence models to
jointly predict all of the translations for a sentence at once
\cite{rudnick-gasser:2013:HyTra}.

Francis Tyers, in his dissertation work \cite{tyers-dissertation}, provides an
overview of lexical selection systems and describes methods for learning
lexical selection rules based on available parallel corpora. These rules make
reference to the lexical items and parts of speech surrounding the word to be
translated. Once learned, these rules intended to be understandable and
modifiable by human language experts; for use in the Apertium machine
translation system, they are compiled to finite-state transducers.

Rios and G\"{o}hring \shortcite{riosgonzales-gohring:2013:HyTra} describe
earlier work extending the SQUOIA MT system, in which they add machine learning
components that can predict the target forms of verbs in cases where the
present rules are not sufficient.

\section{Conclusions and Future Work}
%% XXX
Here we have described the Chipa CL-WSD system and its integration into SQUOIA.
While before, most of SQUOIA's lexical choices were simply based on default
entries in a dictionary or sometimes on a few hand-written lexical selection
rules, now 


\bibliographystyle{lrec2006}
\bibliography{saltmil2014}

\end{document}

