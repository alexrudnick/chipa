\documentclass[10pt, a4paper]{article}
\usepackage{lrec2006}
\usepackage{graphicx}
\usepackage{url}
\usepackage{latexsym}
\usepackage{listings}
\usepackage[utf8]{inputenc}

%% XXX: we need a much better title.
\title{Enhancing a Rule-Based MT System with Cross-Lingual WSD}

%%\name{Author1, Author2, Author3}
%%
%%\address{ Affiliation1, Affiliation2, Affiliation3 \\
%%               Address1, Address2, Address3 \\
%%               author1@xxx.yy, author2@zzz.edu, author3@hhh.com\\}

\abstract{
Lexical ambiguity is a significant problem facing rule-based machine
translation systems, as many words have several possible translations in a
given target language, each of which can be considered a sense of the word from
the source language.
The difficulty of resolving these ambiguities is mitigated for 
statistical machine translation systems for language pairs with large bilingual
corpora, as large n-gram language models and phrase tables containing common
multi-word expressions can encourage coherent word choices.
For most language pairs these resources are not available, so a primarily
rule-based approach becomes attractive.
In cases where some training data is available, though, we can
investigate hybrid RBMT and machine learning approaches, leveraging small and
potentially growing bilingual corpora. In this paper we
describe the integration of statistical cross-lingual word-sense disambiguation
software with SQUOIA, an existing rule-based MT system for the Spanish-Quechua
language pair, and show how it allows us to learn from the available bitext to
make better lexical choices, with very few code changes to the base system. We
also describe Chipa, the new open source CL-WSD software used for these
experiments.
\\ \newline
\Keywords{under-resourced languages, hybrid machine translation, word-sense
disambiguation}}

\begin{document}

\maketitleabstract

\section{Introduction}
Here we report on initial results and our experience in integrating statistical
lexical selection software into an existing rule-based machine translation
(RBMT) system for translating Spanish into Quechua.
With very few code changes, we were able to integrate Chipa, a statistical
cross-lingual WSD system, into
SQUOIA\footnote{\url{http://code.google.com/p/squoia/}},
a transfer-based RBMT system based on the architecture of MATXIN
\cite{matxin_2005,matxin}.

This integration enables SQUOIA to take advantage of any available bitext
without significantly changing its design, and to improve its word choices as
additional bitext becomes available.
In this paper, we describe the designs of the Chipa and SQUOIA systems, discuss
the data sets used, and give results on both how well Chipa is able to learn
lexical selection classifiers in isolation, and to what extent it is able to
improve the output of SQUOIA on a full Spanish-to-Quechua translation task.
%% TODO: quickly summarize results here.

In its current design, SQUOIA gets its word choices purely from a bilingual
dictionary; the candidate translations for each word and multi-word expression
are looked up with \texttt{matxin-lex-xfer}. If there are several possible
translations for a source-language word, passed along to later stages in the
pipeline. While there are some rules for lexical selection, they have
been written by hand and only cover a small subset of the vocabulary.

In this work, we supplement these rules with classifiers learned from
Spanish-Quechua bitext. These classifiers automatically make use of
regularities that might not be obvious to human rule-writers, and are
automatically discovered in the available bitext, allowing appropriate lexical
selections for any word type with adequate coverage in the training corpus.

Quechua is a group of closely related indigenous American languages spoken in
South America. There are many dialects of Quechua, but SQUOIA focuses on
the Cuzco dialect, spoken around the Peruvian city of Cuzco.
This dialect is considered a standard, with its own language academy
(the \emph{Academia Mayor de la Lengua Quechua}), a relatively large number of
speakers, and some useful linguistic resources available, such as a small
treebank \cite{rios2009quechua}, also produced by the SQUOIA team.

\section{The SQUOIA machine translation system}
The core SQUOIA system is mostly rule-based and relies on a classical transfer
approach.
It uses a pipeline approach; the system contains many modules, each of which
perform some transformation on their input and pass along their changes to the
next stage. Many modules leave most of the input unchanged.
In a first step, the Spanish source sentence
is analyzed (morphological analysis, PoS-tagging, named entity recognition,
parsing). In analyzing the Spanish text, SQUOIA uses FreeLing \cite{padro12}
for morphological analysis, Wapiti \cite{lavergne2010practical} for tagging,
and DeSr \cite{attardi-EtAl:2007:EMNLP-CoNLL2007} for parsing. All of
these modules rely on statistical models.

In the next step, the Spanish verbs must be disambiguated in order to assign
them a Quechua verb form for generation: a rule-based module tries to assign a
verb form to each verb chunk based on context information. If the rules fail to
do so due to parsing or tagging errors, the verb is marked as ambiguous and
passed on to an SVM classifier, that assigns a verb form even if the context
information is not complete. This is the most difficult part of the translation
process, as the grammatical categories encoded in verbs differ substantially
between Spanish and Quechua.  In the next step, a lexical transfer module
inserts all possible translations for every word from a bilingual dictionary,
after that, a set of rules disambiguates forms with lexical or morphological
ambiguities. However, this rule-based lexical disambiguation is very limited,
as it is not feasible to cover all possible contexts for every ambiguous word
with rules.

The rest of the system is a classical transfer procedure. A following module
moves syntactic information between the nodes and the chunks in the tree, and
finally, the tree is reordered according to the basic word order in the target
language. In the last step, the Quechua surface forms are morphologically
generated through a finite state transducer.

\section{The Chipa CL-WSD system}
Chipa is a system for cross-language word sense disambiguation (CL-WSD). By
CL-WSD, we mean the problem of assigning labels to polysemous words in
source-language text, where each label is a word or phrase type in the target
language.

This framing of word-sense disambiguation concretely situates CL-WSD as a
useful part of a machine translation system, avoiding entirely the problems
associated with choosing a sense inventory, as the sense distinctions that a
CL-WSD system should learn are exactly those that are lexicalized in the target
language. CL-WSD as an approach additionally sidesteps the ``knowledge
acquisition bottleneck"
\cite{agirre2006word}
hampering much other work in WSD, as it requires bitext, which, while limited
in availability, is more common than sense-annotated text and is useful on its
own for corpus-based MT.

Chipa uses discriminative classifiers, trained individually for source-language
words, in order to distinguish between the senses associated with different
target-language word types.

In this work, we use a number of features extracted ...

The chipa system holds all of the available bitext in a database, waiting for
sentences to translate.


We have to say: well, here are the options. es1 can translate to qu1 or qu2 or
qu3. Which one of these do you like the best, chipa?
What if a qu word is in the lexicon but it's never seen in the training data?
Just always prefer the words that we've seen?


Open question: how much training data do we need for a word before it helps to
do a classifier?
Proposal:
- if never seen it, just don't disambiguate
- if seen it a few times, take the MFS
- if many times, train a classifier!



Features extracted include ...
- syntactic features? we should have a parse/chunking handy!
- bag of words over the whole input sentence?
- local context words?
- foreign-language BOW? Maybe we'll try that later?
- sequence models?




Initial versions of the chipa software are described in
\cite{rudnick-gasser:2013:HyTra}
On demand, chipa either retrieves or trains (then caches) a classifier
for each word in a sentence to be translated.


Classifiers are trained with the scikit-learn machine learning package
\cite{scikit-learn}, and the software uses various utility functions from NLTK
\cite{nltkbook}.


\section{Using word-sense discrimination features for WSD}
We can make use of the many resources available for Spanish in order to make
better sense of the input text, which we hypothesize should be useful in making
word choices in Quechua, or other under-resourced target languages.

One resource available for Spanish is the simple abundance of monolingual text;
given large amounts of Spanish-language text, we can use unsupervised methods
to discover semantic regularities.


The idea here is to use unsupervised methods to learn an annotation scheme by
which we can add additional labels to ...

Brown Clusters were originally described in \cite{Brown92class-basedn-gram},
but have since been used in a variety of other NLP tasks, especially text
classification settings.


\section{Available bitext and data preparation}
For this work, we use an assortment of Spanish-Quechua bitexts.
While there are not very many available large corpora, there are some.

As sentences alignments are not guaranteed to be clean, we use Robert Moore's
sentence aligner \cite{DBLP:conf/amta/Moore02}, with the default settings.
There were THISMANY sentences in the corpora we used, THISMANY of which are
included in the resulting sentence-aligned bitext.

We performed word-level alignments on the remaining sentences with the Berkeley
aligner, resulting in one-to-many alignments such that each Spanish word may be
aligned to zero or more Quechua words. Spanish multi-word expressions known to
the lexicon were replaced with special tokens to mark that particular
expression.

\section{}
%% XXX
* consider: how many instances of a word do we need before it helps to train a
classifier?


Describe the results of initial testing on a held-out test set of es-qu CL-WSD
tasks.


\section{System Integration}
%% XXX
Given the Chipa WSD software and the pipeline design of the SQUOIA MT system,
we were

\section{In-Vivo Experimental Setup}
%% XXX
We prepared a number of example sentence to be translated, and found that
XXXX\% of them had better translations because of better lexical choice.


Hey, maybe words that are in the Spanish wikipedia article linked to a Quechua
wikipedia article are informative features for selecting those Quechua words?
...

\section{Related Work}
%% XXX

There has been some work on CL-WSD for translating into indigenous American
languages; an earlier system for Spanish-Guarani made use of sequence models to
jointly predict all of the translations for a sentence at once
\cite{rudnick-gasser:2013:HyTra}.

Francis Tyers, in his dissertation \cite{tyers-dissertation}, provides an
overview of lexical selection systems and provides methods for learning lexical
selection rules that fit within the %% XXX? 
matxin lexical selection rule framework.

Rios and G\"{o}hring \shortcite{riosgonzales-gohring:2013:HyTra} describe
similar work on extending the SQUOIA MT system, adding machine learning
components that can predict the target forms of verbs in cases where the
present rules are not sufficient 

In work on the Parasense system \cite{lefever-hoste-decock:2011:ACL-HLT2011},
Lefever \emph{et al.} produced top results for CL-WSD translating English nouns
into five different European languages. The classifiers in this work are
trained on standard local contextual features surrounding the target word,
%% XXX: terminology, should we use "target word"?
in addition to a bag-of-words model of the source language sentence in four
other foreign languages. At training time, the translations of the sentence are
extracted from mutually parallel corpora, %% XXX terminology?
but at training time
This work has not yet, to our knowledge, been integrated into a machine
translation system, in part because it relies on an external machine
translation system as part of its feature extraction procedure.

Rudnick \emph{et al.} \shortcite{rudnick-liu-gasser:2013:SemEval-2013}
prototyped a system that addresses some of the shortcomings here, requiring
more modest software infrastructure for feature extraction while still allowing
CL-WSD systems to make use of several mutually parallel bitexts that share a
source language.

\section{Conclusions and Future Work}
%% XXX
Here we have described the Chipa CL-WSD system and its integration into SQUOIA.
While before, most of SQUOIA's lexical choices were simply based on default
entries in a dictionary or sometimes on a few hand-written lexical selection
rules, now 


\bibliographystyle{lrec2006}
\bibliography{saltmil2014}

\end{document}

