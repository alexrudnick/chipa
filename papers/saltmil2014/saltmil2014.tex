\documentclass[10pt, a4paper]{article}
\usepackage{lrec2006}
\usepackage{graphicx}

\title{Integrating CL-WSD into an existing RBMT system for Spanish-Quechua}

%%\name{Author1, Author2, Author3}
%%
%%\address{ Affiliation1, Affiliation2, Affiliation3 \\
%%               Address1, Address2, Address3 \\
%%               author1@xxx.yy, author2@zzz.edu, author3@hhh.com\\}


\abstract{Each article must include an abstract of 150 to 200 words in Times 9
  pt with interlinear spacing of 10 pt.
 The heading Abstract should be centred, font Times 10 bold. This short
 abstract will also be used for printing a Booklet of Abstracts 
containing the abstracts of all papers presented at the Conference. \\ \newline
\Keywords{keyword A, keyword B, keyword C}}



\begin{document}

\maketitleabstract

\section{Introduction}
Here we present initial results from our work on integrating a statistical
lexical selection system into an existing rule-based machine translation (RBMT)
system for translating Spanish into Quechua, an indigenous American language
spoken in the Andes mountains.


With very few code changes, we were able to integrate chipa, a statistical WSD
system, into SQUOIA (cite SQUOIA),
an existing transfer-based RBMT system. This
allows SQUOIA to take advantage of any available bitext without significantly
changing its design, and to improve its word choices as additional bitext
becomes available. We additionally show how these improved lexical selections
result in better translations.


In the existing SQUOIA pipeline, lexical choices are produced from a lexicon
and looked up with the MATXIN system's matxin-lex-xfer command. If there are
several possible translations for a source-language word, they are passed along
in the pipeline. There are currently some lexical selection rules, but they
must be written by hand.


In this work, we supplement these rules with discriminative classifiers learned
for each Spanish-language word, as appropriate, on demand. These classifiers 


\section{the SQUOIA machine translation system}
SQUOIA is described in more detail in (CITATION), but briefly...
(brief enough? feel free to cut unnecessary information out)

We have implemented a hybrid MT system in our research project SQUOIA (link)
that translates Spanish text to Cuzco Quechua. The core system is still mostly
rule-based and relies on a classical transfer approach, where several modules
are joined in a processing chain: In a first step, the Spanish source sentence
is analyzed (morphological analysis, PoS-tagging, named entity recognition,
parsing). We use FreeLing CITE for the morphological analysis, conditional
random fields (Wapiti CITE) for tagging and DeSr CITE for parsing. All of these
modules rely on statistical models. In the next step, the Spanish verbs have to
be disambiguated in order to assign them a Quechua verb form for generation: a
rule-based module tries to assign a verb form to each verb chunk based on
context information. If the rules fail to do so due to parsing or tagging
errors, the verb is marked as ambiguous and passed on to an SVM classifier,
that assigns a verb form even if the context information is not complete. This
is the most difficult part of the translation process, as the grammatical
categories encoded in verbs differ substantially between Spanish and Quechua.
In the next step, a lexical transfer module inserts all possible translations
for every word from a bilingual dictionary, after that, a set of rules
disambiguates forms with lexical or morphological ambiguities. However, this
rule-based lexical disambiguation is very limited, as it is not feasible to
cover all possible contexts for every ambiguous word with rules.

INSERT how does your system work?

The rest is classical transfer procedure: the next module moves syntactic
information between the nodes and the chunks in the tree, and finally, the tree
is reordered according to the basic word order in the target language. In the
last step, the Quechua surface forms are morphologically generated through a
finite state transducer.



- pipeline that builds and then passes along a nested chunk structure, serialized as XML when necessary
- each stage in the pipeline may modify or add to the data structure, but most stages leave most of the tree alone
- look up words (or MWEs!?) in the lexicon
- maybe resolve ambiguities later


We have to say: well, here are the options. es1 can translate to qu1 or qu2 or
qu3. Which one of these do you like the best, chipa?
What if a qu word is in the lexicon but it's never seen in the training data?
Just always prefer the words that we've seen?


Open question: how much training data do we need for a word before it helps to
do a classifier?
Proposal:
- if never seen it, just don't disambiguate
- if seen it a few times, take the MFS
- if many times, train a classifier!


Then the question is what that cutoff for "many" is.



\section{Available bitext and data preparation}
For this work, we use an assortment of es-qu bitexts -- there are not very many
available large corpora...

As sentences alignments are not guaranteed to be clean, we use Bob Moore's
sentence aligner
\cite{DBLP:conf/amta/Moore02}
at a threshold of THETHRESHOLD. There were
THISMANY sentences in the corpora we used, THISMANY of which are included in
the resulting sentence-aligned bitext.


We performed word-level alignments on the remaining sentences with the Berkeley
aligner, resulting in one-to-many alignments such that one Spanish word may be
aligned to many Quechua words. Spanish multiword expressions known to the
lexicon were replaced with special tokens to mark that MWE.


\section{the Chipa CL-WSD system}
Here we describe the structure of the chipa CL-WSD system.


Chipa is an open source CL-WSD system, available at (REDACTED)


In this work, we use a number of features extracted ...


Features extracted include ...
- syntactic features? we should have a parse/chunking handy!
- bag of words over the whole input sentence?
- local context words?
- foreign-language BOW? Maybe we'll try that later?
- sequence models?


The chipa system holds all of the available bitext in a database, waiting for
sentences to translate.


Initial versions of the chipa software are described in
\cite{rudnick-gasser:2013:HyTra}
On demand, chipa either retrieves or trains (then caches) a classifier
for each word in a sentence to be translated.


Classifiers are trained with the scikit-learn machine learning package
\cite{scikit-learn}, and the software uses various utility functions from NLTK
\cite{nltkbook}.


\section{Using word-sense discrimination features for WSD}

We can make use of the many resources available for Spanish in order to make
better sense of the input text, which we hypothesize should be useful in making
word choices in Quechua, or other under-resourced target languages.


One resource available for Spanish is the simple abundance of monolingual text;
given large amounts of Spanish-language text, we can use unsupervised methods
to discover semantic regularities.


The idea here is to use unsupervised methods to learn an annotation scheme by
which we can add additional labels to ...


\subsection{Brown Clusters}
Brown Clusters, originally 




\section{Testing the CL-WSD system on its own}
* consider: how many instances of a word do we need before it helps to train a
classifier?


Describe the results of initial testing on a held-out test set of es-qu CL-WSD
tasks.


\section{In vivo testing: can we do better on the whole translation pipeline?}


We prepared a number of example sentence to be translated, and found that
XXXX\% of them had better translations because of better lexical choice.


Hey, maybe words that are in the Spanish wikipedia article linked to a Quechua
wikipedia article are informative features for selecting those Quechua words?
...


\section{better sequence models}


Last time around, we tried MEMMs that back off to HMMs.


But maybe that's not a very good estimate; if we don't have enough data to
reliably estimate a MaxEnt classifier, how are we going to estimate an HMM?


Maybe the right thing to do is back off to the MFS.




\section{Related Work}
Francis Tyers. Is his dissertation available anywhere?
\cite{tyers-dissertation}


Annette and Anne
using ML to predict verb forms in 2013.
\cite{riosgonzales-gohring:2013:HyTra}


Rudnick and Gasser: CL-WSD for Guarani.


\section{Conclusions and Future Work}
Here we have described the Chipa CL-WSD system and its integration into SQUOIA.
While before, most of SQUOIA's lexical choices were simply based on default
entries in a dictionary or sometimes on a few hand-written lexical selection
rules, now 


\bibliographystyle{lrec2006}
\bibliography{saltmil2014}

\end{document}

